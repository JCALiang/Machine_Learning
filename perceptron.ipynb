{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning – an issue with distance measures, and an introduction to linear classification with the Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ Last revision: Thu Mar 29 11:20:34 AEDT 2018 _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will expand on some of the concepts of linear\n",
    "classification, starting with an experiment with distance measures on data, then looking into the perceptron. Initially we understand the representational capacity of a perceptron, then how to implement learning for elementary Boolean functions, i.e., concept learning, and finally look at a perceptron learning a linear classifier on a real-world dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron implementation for this lab is based on the presentation and code in Chapter 3 of ``Machine Learning'' by Stephen Marsland, CRC Press, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance measures for high-dimensionality data\n",
    "\n",
    "Algorithms such as $k$-Nearest Neighbour are conceptually very simple -- we predict the class value of an unlabelled *query* data point we are given by looking at all the labelled data point(s) in our data set, and predicting that our query will have the same class as the most similar data point(s) in the training set. So, all we need is a way of measuring similarity. The well-known *Euclidean distance measure* would seem to be a good choice. However, while we are very familiar with Euclideam distance in 2 and 3-dimensions, there was a warning (Slide 33 of the \"Classification\" lecture) that in high-dimensions there is a problem – what was this problem ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-330d61df3811>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-330d61df3811>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Answer:\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Pairwise distances in high-dimensional spaces \n",
    "\n",
    "Answer: everyone is far away from one another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what does this actually mean ? There is a mathematical argument to show that this is a true statement, but an alternative approach is simply to simulate what happens. One approach is to randomly generate $N$ points inside a $d$-dimensional cube centred around zero, such as $[-0.5, 0.5]^{d}$. Now take all pairwise distances between these points and plot them. Do this for at least 3 settings for $d$, such as $d=3$, $100$ and $1000$. Plot the pairwise distances using a histogram from the [```matplotlib```](http://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html) library. What do you observe as $d$ is increased ? Does this support the statement above ?\n",
    "\n",
    "You should use the ```numpy``` library for this, and in particular the linear algebra methods to calculate the distance as the [L2 norm](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.norm.html#numpy.linalg.norm). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1225\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd21707b278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "my_dim = 100\n",
    "# my_dim = 100\n",
    "# my_dim = 1000\n",
    "my_pts = 50\n",
    "\n",
    "# Generate 50 random points in the dimensionality chosen (3, 100 or 1000)\n",
    "\n",
    "rand_points= [[round(random.random()-0.5, 2) for _ in range(my_dim)] for _ in range(my_pts) ]\n",
    "\n",
    "\n",
    "# Get the pairwise distance between all pairs of points\n",
    "d_count=[]\n",
    "for i in range(len(rand_points)):\n",
    "    p1=rand_points[i]\n",
    "    for p2 in rand_points[i+1:]:\n",
    "        dist = np.linalg.norm(np.array(p1)-np.array(p2))\n",
    "        d_count.append(dist)\n",
    "        #d_count.append(math.sqrt(total))\n",
    "        \n",
    "        \n",
    "print(len(d_count))\n",
    "\n",
    "\n",
    "# Save all these distances into an array then plot in a histogram\n",
    "n_bins = 10\n",
    "plt.hist(d_count, n_bins, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classification with the Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will use a slight variant on the notation and setup\n",
    "used in the lectures.\n",
    "These changes are not going to affect the capabilities of the perceptron.\n",
    "\n",
    "For a given set of $m$ inputs, the first stage of the computation is when the perceptron  multiplies each of the input values with its corresponding weight and adds these together:\n",
    "\n",
    "$$ h = \\sum_{i}^{m} w_{i} x_{i} $$\n",
    "\n",
    "The second stage is to apply the thresholding output rule or activation function of the perceptron to produce the classification output.\n",
    "\n",
    "For this lab we will slightly change the activation function to map to either $0$ or $1$ rather than the $-1$ or $+1$ we had in the lecture notes.\n",
    "\n",
    "The value set for the bias or threshold input will also be changed from $1$ to $-1$.\n",
    "\n",
    "$$ o = g(h) = \\left\\{\n",
    "                \\begin{array}{lll}\n",
    "                        1 & \\mbox{if}           & h > 0 \\\\\n",
    "                        0 & \\mbox{otherwise if} & h \\leq 0 \\\\\n",
    "                \\end{array}\n",
    "              \\right. $$\n",
    "\n",
    "Let's go ahead and implement a Perceptron in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing simple Boolean functions as a linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first look at modelling a simple two-input Boolean function as linear classifier. This is a Perceptron WITHOUT any learning! To get started we will use the OR function, for which the truth table will be familiar to you all. Note that you will need to pick some weights for the function to output the correct values given the input. There are many possible values that could do the job. Also, remember to take care with the dimension of the weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Input [0, 0] with Class 0 Predict  0\n",
      "For Input [0, 1] with Class 1 Predict  1\n",
      "For Input [1, 0] with Class 1 Predict  1\n",
      "For Input [1, 1] with Class 1 Predict  1\n"
     ]
    }
   ],
   "source": [
    "# set up the data, i.e., all the cases in the truth table \n",
    "x=[[0,0],[0,1],[1,0],[1,1]]\n",
    "y=[0,1,1,1]\n",
    "# number of data points\n",
    "n=4\n",
    "# number of inputs to the perceptron\n",
    "m=3\n",
    "# what weights should be assigned to correctly compute the OR function ?\n",
    "# fill in your weights here by assigning a weight vector to w\n",
    "w=[0,1,1]\n",
    "# how can you come up with a suitable set of weights ?\n",
    "# loop over the data\n",
    "for i in range(n):\n",
    "    h=w[0]*(-1)# this is the bias weight and input\n",
    "    for j in range(1,m):\n",
    "        h+=w[j]*x[i][j-1]\n",
    "    if(h>0):\n",
    "        output=1\n",
    "    else:\n",
    "        output=0\n",
    "    print('For Input', x[i], 'with Class', y[i], 'Predict ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change your code to model the AND function (again restricted to two inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Input [0, 0] with Class 0 Predict  0\n",
      "For Input [0, 1] with Class 0 Predict  0\n",
      "For Input [1, 0] with Class 0 Predict  0\n",
      "For Input [1, 1] with Class 1 Predict  1\n"
     ]
    }
   ],
   "source": [
    "# set up the data, i.e., all the cases in the truth table \n",
    "x=[[0,0],[0,1],[1,0],[1,1]]\n",
    "y=[0,0,0,1]\n",
    "# number of data points\n",
    "n=4\n",
    "# number of inputs to the perceptron\n",
    "m=3\n",
    "# what weights should be assigned to correctly compute the AND function ?\n",
    "# fill in your weights here by assigning a weight vector to w\n",
    "# how can you come up with a suitable set of weights ?\n",
    "# loop over the data\n",
    "w=[0.5,0.5,0.5]\n",
    "for i in range(n):\n",
    "    h=w[0]*(-1)# this is the bias weight and input\n",
    "    for j in range(1,m):\n",
    "        h+=w[j]*x[i][j-1]\n",
    "      \n",
    "    if(h>0):\n",
    "        output=1\n",
    "    else:\n",
    "        output=0\n",
    "    print('For Input', x[i], 'with Class', y[i], 'Predict ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the data structures for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got right down to the details of how a linear classifier works. Now this being a perceptron, you probably recall that rather than using a fixed set of weights to do the prediction each time, there is a simple training rule that updates the weights on the basis of discrepancies between the classifier's prediction on the data and the actual class. So we could extend our previous code to implement that training rule, but the code is a little fiddly and you're probably thinking there should be a simpler way to do this. If so, you are correct, but it is based on moving towards coding with matrix and vector operations, rather than directly using Python arrays. To do this we need to import the NumPy library (there is a tutorial at: <href <a>https://docs.scipy.org/doc/numpy-dev/user/quickstart.html</a>>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, when we need to predict a class for an instance $\\mathbf{x}$ given the current weights $\\mathbf{w}$ we can use the inner product operation $\\mathbf{x} \\cdot \\mathbf{w}$. To get this functionality using NumPy we just do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.array([0,1,1])\n",
    "w=np.array([0.02,0.03,0.03])\n",
    "\n",
    "h=np.dot(x,w)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait, there's more! Since $\\mathbf{x}$ and $\\mathbf{w}$ are both actually matrices, the same operation will enable us to apply the inner product of the weight vector $\\mathbf{w}$ to ALL the data instances at once. In this case we write the matrix of data instances $\\mathbf{X}$. Just note that we need to take care that the data matrix and weight vector are properly initialised to make this operation work correctly. Now the code for predicting the class values of all of our data given the weight vector is as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0. -1.]\n",
      " [ 0.  1. -1.]\n",
      " [ 1.  0. -1.]\n",
      " [ 1.  1. -1.]]\n",
      "Activations:\n",
      " [[-0.02]\n",
      " [ 0.01]\n",
      " [ 0.01]\n",
      " [ 0.04]]\n",
      "Predictions:\n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Misclassifications\n",
      " [[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data set with class values in last column\n",
    "dataset = np.array([[0,0,0],[0,1,1],[1,0,1],[1,1,1]]) # OR function\n",
    "\n",
    "X=dataset[:,0:2]\n",
    "\n",
    "y = dataset[:,2:]\n",
    "# Note: the bias weight is now the last!\n",
    "w = np.array([[0.03],[0.03],[0.02]])\n",
    "# Add the values for the bias weights (-1) to the data matrix\n",
    "nData = np.shape(X)[0]\n",
    "X = np.concatenate((X,-np.ones((nData,1))),axis=1)\n",
    "# get the value of the activation function\n",
    "h = np.dot(X,w)\n",
    "yhat = np.where(h>0,1,0)\n",
    "err = yhat-y\n",
    "\n",
    "print('Activations:\\n', h)\n",
    "print('Predictions:\\n', yhat)\n",
    "print('Misclassifications\\n', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses some more NumPy built-ins. Check the documentation to be sure you know what is going on. One of these, np.where(), is useful here. It takes 3 arguments and returns an array. The first argument is a predicate on an array that is either evaluates to true, returning the second argument at the corresponding index in the array or false, returning the third argument instead. Now see how you get on re-implementing the code to do the prediction for the two-input Boolean AND function, as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations:\n",
      "[[-0.02]\n",
      " [ 0.01]\n",
      " [ 0.01]\n",
      " [ 0.04]]\n",
      "Predictions:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Misclassifications\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data set with class values in last column\n",
    "# dataset for AND function\n",
    "X=dataset[:,0:2]\n",
    "y = dataset[:,2:]\n",
    "# Note: the bias weight is now the last!\n",
    "# fill in your weights here by assigning a weight vector to w\n",
    "# Add the values for the bias weights (-1) to the data matrix\n",
    "nData = np.shape(X)[0]\n",
    "X = np.concatenate((X,-np.ones((nData,1))),axis=1)\n",
    "# get the value of the activation function\n",
    "h = np.dot(X,w)\n",
    "yhat = np.where(h>0,1,0)\n",
    "err = yhat-y\n",
    "\n",
    "print 'Activations:\\n', h\n",
    "print 'Predictions:\\n', yhat\n",
    "print 'Misclassifications\\n', err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in weight updates to make the learning work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have spent some time just getting the weights and data in the right vector-matrix format to be able to do the prediction. What else do we need to get this thing to learn ?\n",
    "\n",
    "One thing we will need is some random initialisation for the weight vector. What sort of values would be appropriate for this initialisation?\n",
    "\n",
    "The initialisation will be done using a NumPy built-in. Note that we need weights for each of the inputs \"nIn\", plus one for the bias. Also, the \"nOut\" parameter is just a placeholder in case you want your Perceptron to predict more that one output at a time. Here we will just use one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0116604 ]\n",
      " [ 0.03216475]\n",
      " [ 0.03009196]]\n"
     ]
    }
   ],
   "source": [
    "nIn = 2    # still working with 2-input Boolean functions\n",
    "nOut = 1   # so a true/false classification output\n",
    "w = np.random.rand(nIn+1,nOut)*0.1-0.05 # Check: does this return a column vector?\n",
    "print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other main thing we need is to see how the Perceptron training rule is implemented to update the weights for each attribute given all the information in the data matrix plus the misclassifications. Note that this implementation is a batch version, unlike the version in the lecture notes which is incremental. Both approaches have their place. Here we go for simplicity of implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What must the inner dimensions of the matrix multiplcation be for the weight update ? Check with the lecture notes to see what terms we will need. Recall that the augmented data matrix has $m+1$ columns, where $m$ is the number of inputs. However, the misclassifications, or errors, are of dimensionality $n$, because there is potentially one misclassification for every example in the dataset. What has to happen ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct: you need to transpose the augmented data matrix to ensure the inner dimensions match (they both must be of size $n$). Check you are sure before inspecting the code (it's just a one-liner). Here the parameter \"eta\" is the learning rate $\\eta$, which for this code is set to $0.25$. Once more $\\hat{y} - y$ will be our misclassification vector. Can you see why the updated weight vector $w$ has the values it does ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0116604 ]\n",
      " [ 0.03216475]\n",
      " [ 0.03009196]]\n"
     ]
    }
   ],
   "source": [
    "eta=0.25\n",
    "w -= eta*np.dot(np.transpose(X),yhat-y)# this is it - learning in one line of code!\n",
    "print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put it all together. Note that we need to set an upper limit for the number of iterations (T). Play with this code and run it as above for our Boolean functions. See what happens to the weights for \"OR\". Does the Perceptron learn this function? Now try \"AND\". Then try \"XOR\" (exclusive or). Now go back and experiment with the learning rate. Does anything change ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0  Error: 0.75\n",
      "Iteration: 1  Error: 0.25\n",
      "Iteration: 2  Error: 0.25\n",
      "Iteration: 3  Error: 0.0\n",
      "Iteration: 4  Error: 0.0\n",
      "Iteration: 5  Error: 0.0\n",
      "Iteration: 6  Error: 0.0\n",
      "Iteration: 7  Error: 0.0\n",
      "Iteration: 8  Error: 0.0\n",
      "Iteration: 9  Error: 0.0\n",
      "Iteration: 10  Error: 0.0\n",
      "Iteration: 11  Error: 0.0\n",
      "Iteration: 12  Error: 0.0\n",
      "Iteration: 13  Error: 0.0\n",
      "Iteration: 14  Error: 0.0\n",
      "Iteration: 15  Error: 0.0\n",
      "Iteration: 16  Error: 0.0\n",
      "Iteration: 17  Error: 0.0\n",
      "Iteration: 18  Error: 0.0\n",
      "Iteration: 19  Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "# Dataset with class values in last column\n",
    "#dataset = np.array([[0,0,0],[0,1,1],[1,0,1],[1,1,1]])   # OR function\n",
    "# dataset for AND function\n",
    "# dataset for XOR function\n",
    "X = dataset[:,0:2]\n",
    "y = dataset[:,2:]\n",
    "nIn = np.shape(X)[1]    # no. of columns of data matrix\n",
    "nOut = np.shape(y)[1]   # no. of columns of class values -- just 1 here\n",
    "nData = np.shape(X)[0]  # no. of rows of data matrix\n",
    "w = np.random.rand(nIn+1,nOut)*0.1-0.05\n",
    "X = np.concatenate((X,-np.ones((nData,1))),axis=1)\n",
    "eta=0.25\n",
    "T=20\n",
    "# Train for T iterations\n",
    "for t in range(T):\n",
    "        # Predict outputs given current weights\n",
    "        h = np.dot(X,w)\n",
    "        yhat = np.where(h>0,1,0)\n",
    "        # Update weights for all incorrect classifications\n",
    "        w -= eta*np.dot(np.transpose(X),yhat-y)\n",
    "        # Output current performance\n",
    "        errors=yhat-y\n",
    "        perrors=((nData - np.sum(np.where(errors==0,1,0)))/nData)\n",
    "        #print perrors, 'is Error on iteration:', t\n",
    "        print('Iteration:', t, ' Error:', perrors)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron training on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, try this out on a real dataset, the Pima diabetes dataset. You can download this from within your program. The rest of your program should work the same. Replace the lines defining the dataset, X and y variables with the code below. You might want to increase the number of iterations from 20 as well. Unfortunately, this basic implementation of neural learning is not likely to find a very good model. Try transforming the data, for example, by make all attribute values lie in the same range. Search for methods of normalisation using the NumPy built-in functions \"np.mean()\" and \"np.var()\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "# URL for a copy of the Pima Indians Diabetes dataset (UCI Machine Learning Repository)\n",
    "url = \"http://cse.unsw.edu.au/~mike/comp9417/data/uci_pima_indians_diabetes.csv\"\n",
    "# download the file\n",
    "raw_data = urllib.urlopen(url)\n",
    "# load the CSV file as a numpy matrix\n",
    "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
    "print(dataset.shape) # 8 attributes, 1 class, 768 examples\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8:9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
      " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
      " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
      " ...\n",
      " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
      " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
      " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n",
      "Iteration: 0  Error: 0.3489583333333333\n",
      "Iteration: 1  Error: 0.6510416666666666\n",
      "Iteration: 2  Error: 0.3489583333333333\n",
      "Iteration: 3  Error: 0.6510416666666666\n",
      "Iteration: 4  Error: 0.3489583333333333\n",
      "Iteration: 5  Error: 0.4908854166666667\n",
      "Iteration: 6  Error: 0.3489583333333333\n",
      "Iteration: 7  Error: 0.6510416666666666\n",
      "Iteration: 8  Error: 0.3489583333333333\n",
      "Iteration: 9  Error: 0.5572916666666666\n",
      "Iteration: 10  Error: 0.3489583333333333\n",
      "Iteration: 11  Error: 0.38671875\n",
      "Iteration: 12  Error: 0.3528645833333333\n",
      "Iteration: 13  Error: 0.6510416666666666\n",
      "Iteration: 14  Error: 0.3489583333333333\n",
      "Iteration: 15  Error: 0.6497395833333334\n",
      "Iteration: 16  Error: 0.3489583333333333\n",
      "Iteration: 17  Error: 0.3528645833333333\n",
      "Iteration: 18  Error: 0.6497395833333334\n",
      "Iteration: 19  Error: 0.3489583333333333\n",
      "Iteration: 20  Error: 0.58984375\n",
      "Iteration: 21  Error: 0.3489583333333333\n",
      "Iteration: 22  Error: 0.3385416666666667\n",
      "Iteration: 23  Error: 0.6497395833333334\n",
      "Iteration: 24  Error: 0.3489583333333333\n",
      "Iteration: 25  Error: 0.3606770833333333\n",
      "Iteration: 26  Error: 0.3528645833333333\n",
      "Iteration: 27  Error: 0.6497395833333334\n",
      "Iteration: 28  Error: 0.3489583333333333\n",
      "Iteration: 29  Error: 0.6223958333333334\n",
      "Iteration: 30  Error: 0.3489583333333333\n",
      "Iteration: 31  Error: 0.3528645833333333\n",
      "Iteration: 32  Error: 0.6497395833333334\n",
      "Iteration: 33  Error: 0.3489583333333333\n",
      "Iteration: 34  Error: 0.5716145833333334\n",
      "Iteration: 35  Error: 0.3489583333333333\n",
      "Iteration: 36  Error: 0.3723958333333333\n",
      "Iteration: 37  Error: 0.3489583333333333\n",
      "Iteration: 38  Error: 0.6497395833333334\n",
      "Iteration: 39  Error: 0.3489583333333333\n",
      "Iteration: 40  Error: 0.58203125\n",
      "Iteration: 41  Error: 0.3489583333333333\n",
      "Iteration: 42  Error: 0.3229166666666667\n",
      "Iteration: 43  Error: 0.6419270833333334\n",
      "Iteration: 44  Error: 0.3489583333333333\n",
      "Iteration: 45  Error: 0.3268229166666667\n",
      "Iteration: 46  Error: 0.5143229166666666\n",
      "Iteration: 47  Error: 0.3489583333333333\n",
      "Iteration: 48  Error: 0.4739583333333333\n",
      "Iteration: 49  Error: 0.3489583333333333\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request\n",
    "# URL for a copy of the Pima Indians Diabetes dataset (UCI Machine Learning Repository)\n",
    "url = \"http://cse.unsw.edu.au/~mike/comp9417/data/uci_pima_indians_diabetes.csv\"\n",
    "# download the file\n",
    "dataset= np.loadtxt(open(\"uci.csv\", \"rb\"), delimiter=\",\")\n",
    "\n",
    "# load the CSV file as a numpy matrix\n",
    "\n",
    "X = dataset[:,0:8]\n",
    "print(X)\n",
    "y = dataset[:,8:9]\n",
    "\n",
    "nIn = np.shape(X)[1]    # no. of columns of data matrix\n",
    "nOut = np.shape(y)[1]   # no. of columns of class values -- just 1 here\n",
    "nData = np.shape(X)[0]  # no. of rows of data matrix\n",
    "w = np.random.rand(nIn+1,nOut)*0.1-0.05\n",
    "X = np.concatenate((X,-np.ones((nData,1))),axis=1)\n",
    "eta=0.25\n",
    "T=50\n",
    "# Train for T iterations\n",
    "for t in range(T):\n",
    "        # Predict outputs given current weights\n",
    "        h = np.dot(X,w)\n",
    "        yhat = np.where(h>0,1,0)\n",
    "        # Update weights for all incorrect classifications\n",
    "        w -= eta*np.dot(np.transpose(X),yhat-y)\n",
    "        # Output current performance\n",
    "        errors=yhat-y\n",
    "        perrors=((nData - np.sum(np.where(errors==0,1,0)))/nData)\n",
    "        #print perrors, 'is Error on iteration:', t\n",
    "        print('Iteration:', t, ' Error:', perrors)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, this code is just Python code so you can use this for further experimentation. Of course, it can be improved a lot!  A good starting point would be the NumPy documentation."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
